"""
ìŠ¤íŒŒí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ë‘¡ì— ì €ì¥ëœ ì„ë² ë”© ë°ì´í„°ë¡œ ì‚¬ìš©ì ê²Œì‹œê¸€ê³¼ ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ì„ ì°¾ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import os
import sys
import logging
import json
import numpy as np
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col, lit, array
from pyspark.sql.types import FloatType, StringType, StructType, StructField, ArrayType, MapType
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.ml.feature import VectorAssembler
import pymysql
import requests
from io import BytesIO
from PIL import Image

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ìƒìœ„ ë””ë ‰í† ë¦¬ ì¶”ê°€í•˜ì—¬ ëª¨ë“ˆ ì„í¬íŠ¸ ê°€ëŠ¥í•˜ê²Œ í•¨
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from models.clip_model import KoreanCLIPModel
from config import CLIP_MODEL_NAME, DEVICE, SIMILARITY_THRESHOLD

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# MySQL ì—°ê²° ì„¤ì •
MYSQL_HOST = os.getenv('MYSQL_HOST', 'localhost')
MYSQL_PORT = int(os.getenv('MYSQL_PORT', 3306))
MYSQL_USER = os.getenv('MYSQL_USER', 'root')
MYSQL_PASSWORD = os.getenv('MYSQL_PASSWORD', '')
MYSQL_DB = os.getenv('MYSQL_DB', 'lostfound_db')

# í•˜ë‘¡ ì„¤ì •
HADOOP_HOST = os.getenv('HADOOP_HOST', 'http://ec2-x-x-x-x.compute-1.amazonaws.com')
HADOOP_PORT = os.getenv('HADOOP_PORT', '9870')
HADOOP_USER = os.getenv('HADOOP_USER', 'hadoop')
HADOOP_EMBEDDINGS_DIR = os.getenv('HADOOP_EMBEDDINGS_DIR', '/user/hadoop/embeddings')

# ìŠ¤íŒŒí¬ ì„¸ì…˜ ì´ˆê¸°í™”
def create_spark_session():
    """
    ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„± ë° ì´ˆê¸°í™”
    
    Returns:
        pyspark.sql.SparkSession: ìŠ¤íŒŒí¬ ì„¸ì…˜ ê°ì²´
    """
    try:
        logger.info("ìŠ¤íŒŒí¬ ì„¸ì…˜ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        
        # ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„±
        spark = SparkSession.builder \
            .appName("LostFoundSimilarityMatcher") \
            .config("spark.executor.memory", "4g") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.cores", "2") \
            .config("spark.hadoop.fs.defaultFS", f"hdfs://{HADOOP_HOST.replace('http://', '')}:{HADOOP_PORT}") \
            .config("spark.hadoop.yarn.resourcemanager.hostname", HADOOP_HOST.replace('http://', '')) \
            .getOrCreate()
            
        logger.info("ìŠ¤íŒŒí¬ ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
        return spark
    
    except Exception as e:
        logger.error(f"í•˜ë‘¡ì—ì„œ ì„ë² ë”© ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {}, {}

# ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ (ìŠ¤íŒŒí¬ UDFìš©)
def calculate_cosine_similarity(vec1, vec2):
    """
    ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        vec1 (list): ì²« ë²ˆì§¸ ë²¡í„°
        vec2 (list): ë‘ ë²ˆì§¸ ë²¡í„°
        
    Returns:
        float: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0~1 ì‚¬ì´)
    """
    try:
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        a = np.array(vec1)
        b = np.array(vec2)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        dot_product = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        
        if norm_a == 0 or norm_b == 0:
            return 0.0
        
        similarity = dot_product / (norm_a * norm_b)
        
        # ìœ ì‚¬ë„ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” -1~1 ë²”ìœ„)
        normalized_similarity = (similarity + 1) / 2
        
        return float(normalized_similarity)
    
    except Exception as e:
        logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return 0.0

# ì‚¬ìš©ì ê²Œì‹œê¸€ ì„ë² ë”© ìƒì„±
def generate_user_post_embeddings(user_post, clip_model):
    """
    ì‚¬ìš©ì ê²Œì‹œê¸€ì˜ ì„ë² ë”© ìƒì„±
    
    Args:
        user_post (dict): ì‚¬ìš©ì ê²Œì‹œê¸€ ì •ë³´
        clip_model (KoreanCLIPModel): CLIP ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        tuple: (í…ìŠ¤íŠ¸ ì„ë² ë”©, ì´ë¯¸ì§€ ì„ë² ë”©)
    """
    try:
        logger.info("ì‚¬ìš©ì ê²Œì‹œê¸€ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° ê²°í•© (ì¹´í…Œê³ ë¦¬, ë¬¼í’ˆëª…, ìƒ‰ìƒ, ë‚´ìš©)
        text_parts = []
        
        if 'category' in user_post and user_post['category']:
            text_parts.append(f"ì¹´í…Œê³ ë¦¬: {user_post['category']}")
        
        if 'item_name' in user_post and user_post['item_name']:
            text_parts.append(f"ë¬¼í’ˆëª…: {user_post['item_name']}")
        
        if 'color' in user_post and user_post['color']:
            text_parts.append(f"ìƒ‰ìƒ: {user_post['color']}")
        
        if 'content' in user_post and user_post['content']:
            text_parts.append(f"ë‚´ìš©: {user_post['content']}")
        
        combined_text = " ".join(text_parts)
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        text_embedding = clip_model.encode_text(combined_text)[0].tolist()
        
        # ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± (ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)
        image_embedding = None
        
        if 'image_url' in user_post and user_post['image_url']:
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            try:
                image_url = user_post['image_url']
                
                # URLì´ ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì²˜ë¦¬
                if not image_url.startswith(('http://', 'https://')):
                    # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
                    if os.path.exists(image_url):
                        image = Image.open(image_url).convert('RGB')
                    else:
                        logger.warning(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {image_url}")
                        image = None
                else:
                    # URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                    response = requests.get(image_url, timeout=10)
                    if response.status_code == 200:
                        image = Image.open(BytesIO(response.content)).convert('RGB')
                    else:
                        logger.warning(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (HTTP {response.status_code}): {image_url}")
                        image = None
                
                # ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±
                if image is not None:
                    image_embedding = clip_model.encode_image(image)[0].tolist()
                    logger.info("ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
                
            except Exception as e:
                logger.warning(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        logger.info("ì‚¬ìš©ì ê²Œì‹œê¸€ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        return text_embedding, image_embedding
    
    except Exception as e:
        logger.error(f"ì‚¬ìš©ì ê²Œì‹œê¸€ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None, None

# ì„ë² ë”© ë°ì´í„°ë¥¼ ìŠ¤íŒŒí¬ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
def create_embeddings_dataframe(spark, text_embeddings, image_embeddings, lost_items_df):
    """
    ì„ë² ë”© ë°ì´í„°ë¥¼ ìŠ¤íŒŒí¬ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    
    Args:
        spark (pyspark.sql.SparkSession): ìŠ¤íŒŒí¬ ì„¸ì…˜ ê°ì²´
        text_embeddings (dict): í…ìŠ¤íŠ¸ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬
        image_embeddings (dict): ì´ë¯¸ì§€ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬
        lost_items_df (pandas.DataFrame): ë¶„ì‹¤ë¬¼ ë°ì´í„°
        
    Returns:
        pyspark.sql.DataFrame: ì„ë² ë”© ë°ì´í„°ê°€ í¬í•¨ëœ ìŠ¤íŒŒí¬ ë°ì´í„°í”„ë ˆì„
    """
    try:
        logger.info("ì„ë² ë”© ë°ì´í„°ë¥¼ ìŠ¤íŒŒí¬ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        
        # ë²¡í„° ë³€í™˜ í•¨ìˆ˜ ì •ì˜
        def to_vector(embedding):
            return Vectors.dense(embedding)
        
        # ìŠ¤íŒŒí¬ UDF ë“±ë¡
        to_vector_udf = udf(to_vector, VectorUDT())
        
        # ë°ì´í„°í”„ë ˆì„ ìŠ¤í‚¤ë§ˆ ì •ì˜
        schema = StructType([
            StructField("item_id", StringType(), True),
            StructField("text_embedding_array", ArrayType(FloatType()), True),
            StructField("has_image", StringType(), True),
            StructField("image_embedding_array", ArrayType(FloatType()), True)
        ])
        
        # ì„ë² ë”© ë°ì´í„° ìƒì„±
        embeddings_data = []
        
        for item_id, text_emb in text_embeddings.items():
            item_id_str = str(item_id)
            has_image = item_id_str in image_embeddings
            image_emb = image_embeddings.get(item_id_str, None)
            
            embeddings_data.append((item_id_str, text_emb, "yes" if has_image else "no", image_emb))
        
        # ìŠ¤íŒŒí¬ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        embeddings_df = spark.createDataFrame(embeddings_data, schema)
        
        # ë²¡í„° ë³€í™˜
        embeddings_df = embeddings_df.withColumn("text_embedding", to_vector_udf("text_embedding_array"))
        embeddings_df = embeddings_df.withColumn("image_embedding", 
                                                to_vector_udf("image_embedding_array") if "yes" else lit(None))
        
        # ë¶„ì‹¤ë¬¼ ë°ì´í„°ë¥¼ ìŠ¤íŒŒí¬ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        lost_items_spark_df = spark.createDataFrame(lost_items_df)
        
        # ì„ë² ë”© ë°ì´í„°ì™€ ë¶„ì‹¤ë¬¼ ë°ì´í„° ì¡°ì¸
        result_df = embeddings_df.join(lost_items_spark_df, 
                                      embeddings_df.item_id == lost_items_spark_df.id, 
                                      "inner")
        
        logger.info(f"ìŠ¤íŒŒí¬ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ (ë ˆì½”ë“œ ìˆ˜: {result_df.count()})")
        return result_df
    
    except Exception as e:
        logger.error(f"ìŠ¤íŒŒí¬ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise

# ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ì°¾ê¸°
def find_similar_items(spark, user_post, text_embedding, image_embedding, embeddings_df, threshold=SIMILARITY_THRESHOLD):
    """
    ì‚¬ìš©ì ê²Œì‹œê¸€ê³¼ ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ì°¾ê¸°
    
    Args:
        spark (pyspark.sql.SparkSession): ìŠ¤íŒŒí¬ ì„¸ì…˜ ê°ì²´
        user_post (dict): ì‚¬ìš©ì ê²Œì‹œê¸€ ì •ë³´
        text_embedding (list): ì‚¬ìš©ì ê²Œì‹œê¸€ í…ìŠ¤íŠ¸ ì„ë² ë”©
        image_embedding (list): ì‚¬ìš©ì ê²Œì‹œê¸€ ì´ë¯¸ì§€ ì„ë² ë”© (ì—†ìœ¼ë©´ None)
        embeddings_df (pyspark.sql.DataFrame): ì„ë² ë”© ë°ì´í„°ê°€ í¬í•¨ëœ ìŠ¤íŒŒí¬ ë°ì´í„°í”„ë ˆì„
        threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’
        
    Returns:
        list: ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ëª©ë¡ (ìœ ì‚¬ë„ ë†’ì€ ìˆœ)
    """
    try:
        logger.info(f"ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ì°¾ëŠ” ì¤‘... (ì„ê³„ê°’: {threshold})")
        
        # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° UDF ì •ì˜
        calculate_text_similarity_udf = udf(
            lambda emb: calculate_cosine_similarity(text_embedding, emb), 
            FloatType()
        )
        
        # ì„ì‹œ ë·° ìƒì„±
        embeddings_df.createOrReplaceTempView("embeddings")
        
        # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
        result_df = embeddings_df.withColumn("text_similarity", 
                                           calculate_text_similarity_udf("text_embedding_array"))
        
        # ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚° (ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)
        if image_embedding is not None:
            calculate_image_similarity_udf = udf(
                lambda emb: calculate_cosine_similarity(image_embedding, emb) if emb else 0.0, 
                FloatType()
            )
            
            result_df = result_df.withColumn("image_similarity", 
                                           calculate_image_similarity_udf("image_embedding_array"))
            
            # ì¢…í•© ìœ ì‚¬ë„ ê³„ì‚° (í…ìŠ¤íŠ¸ 70%, ì´ë¯¸ì§€ 30%)
            result_df = result_df.withColumn("total_similarity", 
                                           (col("text_similarity") * 0.7) + (col("image_similarity") * 0.3))
        else:
            # ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš° í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë§Œ ì‚¬ìš©
            result_df = result_df.withColumn("image_similarity", lit(0.0))
            result_df = result_df.withColumn("total_similarity", col("text_similarity"))
        
        # ì„ê³„ê°’ ì´ìƒì¸ í•­ëª©ë§Œ í•„í„°ë§
        filtered_df = result_df.filter(col("total_similarity") >= threshold)
        
        # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_df = filtered_df.orderBy(col("total_similarity").desc())
        
        # ê²°ê³¼ ë°ì´í„° ìˆ˜ì§‘
        results = sorted_df.select(
            "id", "category", "item_name", "color", "content", "image_url",
            "text_similarity", "image_similarity", "total_similarity"
        ).collect()
        
        # ê²°ê³¼ ëª©ë¡ ë³€í™˜
        similar_items = []
        
        for row in results:
            item = {
                "id": row["id"],
                "category": row["category"],
                "item_name": row["item_name"],
                "color": row["color"],
                "content": row["content"],
                "image_url": row["image_url"],
                "similarity": {
                    "text": float(row["text_similarity"]),
                    "image": float(row["image_similarity"]),
                    "total": float(row["total_similarity"])
                }
            }
            
            similar_items.append(item)
        
        logger.info(f"ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ {len(similar_items)}ê°œ ì°¾ìŒ")
        return similar_items
    
    except Exception as e:
        logger.error(f"ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ì°¾ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return []

# ë©”ì¸ í•¨ìˆ˜: ì‚¬ìš©ì ê²Œì‹œê¸€ê³¼ ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ì°¾ê¸°
def find_similar_lost_items(user_post, threshold=SIMILARITY_THRESHOLD, limit=10):
    """
    ì‚¬ìš©ì ê²Œì‹œê¸€ê³¼ ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ì°¾ê¸°
    
    Args:
        user_post (dict): ì‚¬ìš©ì ê²Œì‹œê¸€ ì •ë³´
        threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’
        limit (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
    Returns:
        dict: ë§¤ì¹­ ê²°ê³¼
    """
    try:
        logger.info("ì‚¬ìš©ì ê²Œì‹œê¸€ê³¼ ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ì°¾ê¸° ì‹œì‘")
        
        # CLIP ëª¨ë¸ ì´ˆê¸°í™”
        clip_model = initialize_clip_model()
        if clip_model is None:
            return {
                "success": False,
                "message": "CLIP ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨",
                "matches": []
            }
        
        # ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„±
        spark = create_spark_session()
        
        # í•˜ë‘¡ì—ì„œ ìµœì‹  ì„ë² ë”© ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        embeddings_path = get_latest_embeddings_path(spark)
        if not embeddings_path:
            return {
                "success": False,
                "message": "í•˜ë‘¡ì—ì„œ ì„ë² ë”© ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "matches": []
            }
        
        # í•˜ë‘¡ì—ì„œ ì„ë² ë”© ë°ì´í„° ë¡œë“œ
        text_embeddings, image_embeddings = load_embeddings_from_hadoop(spark, embeddings_path)
        if not text_embeddings:
            return {
                "success": False,
                "message": "í•˜ë‘¡ì—ì„œ ì„ë² ë”© ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "matches": []
            }
        
        # MySQLì—ì„œ ë¶„ì‹¤ë¬¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        lost_items_df = fetch_lost_items_from_mysql()
        if lost_items_df.empty:
            return {
                "success": False,
                "message": "MySQLì—ì„œ ë¶„ì‹¤ë¬¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "matches": []
            }
        
        # ì„ë² ë”© ë°ì´í„°ë¥¼ ìŠ¤íŒŒí¬ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        embeddings_df = create_embeddings_dataframe(spark, text_embeddings, image_embeddings, lost_items_df)
        
        # ì‚¬ìš©ì ê²Œì‹œê¸€ ì„ë² ë”© ìƒì„±
        user_text_embedding, user_image_embedding = generate_user_post_embeddings(user_post, clip_model)
        if user_text_embedding is None:
            return {
                "success": False,
                "message": "ì‚¬ìš©ì ê²Œì‹œê¸€ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨",
                "matches": []
            }
        
        # ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ì°¾ê¸°
        similar_items = find_similar_items(
            spark, user_post, user_text_embedding, user_image_embedding, embeddings_df, threshold
        )
        
        # ê²°ê³¼ ì œí•œ
        similar_items = similar_items[:limit]
        
        # ìŠ¤íŒŒí¬ ì„¸ì…˜ ì¢…ë£Œ
        spark.stop()
        
        # ê²°ê³¼ ë°˜í™˜
        return {
            "success": True,
            "message": f"{len(similar_items)}ê°œì˜ ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤",
            "threshold": threshold,
            "total_matches": len(similar_items),
            "matches": similar_items
        }
    
    except Exception as e:
        logger.error(f"ë¶„ì‹¤ë¬¼ ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            "success": False,
            "message": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "matches": []
        }

# FastAPIì—ì„œ í˜¸ì¶œí•˜ëŠ” ë§¤ì¹­ í•¨ìˆ˜
def match_lost_items(user_post, threshold=SIMILARITY_THRESHOLD, limit=10):
    """
    FastAPIì—ì„œ í˜¸ì¶œí•˜ëŠ” ë§¤ì¹­ í•¨ìˆ˜
    
    Args:
        user_post (dict): ì‚¬ìš©ì ê²Œì‹œê¸€ ì •ë³´
        threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’
        limit (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
    Returns:
        dict: ë§¤ì¹­ ê²°ê³¼
    """
    return find_similar_lost_items(user_post, threshold, limit)

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ë¶„ì‹¤ë¬¼ ìœ ì‚¬ë„ ë§¤ì¹­')
    parser.add_argument('--threshold', type=float, default=SIMILARITY_THRESHOLD, help='ìœ ì‚¬ë„ ì„ê³„ê°’')
    parser.add_argument('--limit', type=int, default=10, help='ìµœëŒ€ ê²°ê³¼ ìˆ˜')
    parser.add_argument('--category', type=str, default='ì§€ê°‘', help='ë¶„ì‹¤ë¬¼ ì¹´í…Œê³ ë¦¬')
    parser.add_argument('--item-name', type=str, default='ê²€ì€ìƒ‰ ê°€ì£½ ì§€ê°‘', help='ë¬¼í’ˆëª…')
    parser.add_argument('--color', type=str, default='ê²€ì •ìƒ‰', help='ë¬¼í’ˆ ìƒ‰ìƒ')
    parser.add_argument('--content', type=str, default='ì§€ê°‘ì„ ìƒì–´ë²„ë ¸ìŠµë‹ˆë‹¤. í˜„ê¸ˆê³¼ ì¹´ë“œê°€ ë“¤ì–´ìˆì–´ìš”.', help='ê²Œì‹œê¸€ ë‚´ìš©')
    parser.add_argument('--image', type=str, default=None, help='ì´ë¯¸ì§€ ê²½ë¡œ ë˜ëŠ” URL')
    
    args = parser.parse_args()
    
    # í…ŒìŠ¤íŠ¸ìš© ì‚¬ìš©ì ê²Œì‹œê¸€
    test_post = {
        "category": args.category,
        "item_name": args.item_name,
        "color": args.color,
        "content": args.content,
        "image_url": args.image
    }
    
    # ìœ ì‚¬í•œ ë¶„ì‹¤ë¬¼ ì°¾ê¸°
    result = find_similar_lost_items(test_post, args.threshold, args.limit)
    
    # ê²°ê³¼ ì¶œë ¥
    if result["success"]:
        print(f"ğŸ‰ {result['message']}")
        print(f"ì„ê³„ê°’: {result['threshold']}, ì°¾ì€ í•­ëª© ìˆ˜: {result['total_matches']}")
        
        for i, item in enumerate(result["matches"]):
            print(f"\nâœ… ìœ ì‚¬ í•­ëª© #{i+1}")
            print(f"ID: {item['id']}")
            print(f"ì¹´í…Œê³ ë¦¬: {item['category']}")
            print(f"ë¬¼í’ˆëª…: {item['item_name']}")
            print(f"ìƒ‰ìƒ: {item['color']}")
            print(f"ë‚´ìš©: {item['content'][:100]}..." if len(item['content']) > 100 else f"ë‚´ìš©: {item['content']}")
            print(f"ìœ ì‚¬ë„: í…ìŠ¤íŠ¸ {item['similarity']['text']:.2f}, ì´ë¯¸ì§€ {item['similarity']['image']:.2f}, ì¢…í•© {item['similarity']['total']:.2f}")
    else:
        print(f"âŒ {result['message']}")
        logger.error(f"ìŠ¤íŒŒí¬ ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise

# CLIP ëª¨ë¸ ì´ˆê¸°í™”
def initialize_clip_model():
    """CLIP ëª¨ë¸ ì´ˆê¸°í™”"""
    try:
        logger.info("CLIP ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        model = KoreanCLIPModel(model_name=CLIP_MODEL_NAME, device=DEVICE)
        logger.info("CLIP ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        return model
    except Exception as e:
        logger.error(f"CLIP ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# MySQLì—ì„œ ë¶„ì‹¤ë¬¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
def fetch_lost_items_from_mysql():
    """
    MySQLì—ì„œ ë¶„ì‹¤ë¬¼ ê²Œì‹œê¸€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    
    Returns:
        pandas.DataFrame: ë¶„ì‹¤ë¬¼ ë°ì´í„°
    """
    try:
        logger.info(f"MySQLì—ì„œ ë¶„ì‹¤ë¬¼ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘... ({MYSQL_HOST}:{MYSQL_PORT})")
        
        # MySQL ì—°ê²°
        connection = pymysql.connect(
            host=MYSQL_HOST,
            port=MYSQL_PORT,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DB,
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )
        
        with connection:
            with connection.cursor() as cursor:
                cursor.execute("SELECT * FROM lost_items")
                data = cursor.fetchall()
            
            # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame(data)
            
            logger.info(f"MySQLì—ì„œ {len(df)}ê°œì˜ ë¶„ì‹¤ë¬¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            return df
    
    except Exception as e:
        logger.error(f"MySQL ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return pd.DataFrame()

# í•˜ë‘¡ì—ì„œ ìµœì‹  ì„ë² ë”© ë°ì´í„° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
def get_latest_embeddings_path(spark):
    """
    í•˜ë‘¡ì—ì„œ ìµœì‹  ì„ë² ë”© ë°ì´í„° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    
    Args:
        spark (pyspark.sql.SparkSession): ìŠ¤íŒŒí¬ ì„¸ì…˜ ê°ì²´
        
    Returns:
        str: ìµœì‹  ì„ë² ë”© ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    try:
        logger.info(f"í•˜ë‘¡ì—ì„œ ìµœì‹  ì„ë² ë”© ê²½ë¡œ ì°¾ëŠ” ì¤‘... ({HADOOP_EMBEDDINGS_DIR})")
        
        # í•˜ë‘¡ íŒŒì¼ì‹œìŠ¤í…œ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
        hadoop_fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
        
        # ì„ë² ë”© ë””ë ‰í† ë¦¬ ê²½ë¡œ
        path = spark._jvm.org.apache.hadoop.fs.Path(HADOOP_EMBEDDINGS_DIR)
        
        # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not hadoop_fs.exists(path):
            logger.error(f"í•˜ë‘¡ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {HADOOP_EMBEDDINGS_DIR}")
            return None
        
        # ë””ë ‰í† ë¦¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        status_list = hadoop_fs.listStatus(path)
        
        # ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if not status_list or len(status_list) == 0:
            logger.error(f"í•˜ë‘¡ ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {HADOOP_EMBEDDINGS_DIR}")
            return None
        
        # ìµœì‹  ë””ë ‰í† ë¦¬ ì°¾ê¸°
        latest_dir = None
        latest_time = 0
        
        for status in status_list:
            if status.isDirectory():
                dir_name = status.getPath().getName()
                modify_time = status.getModificationTime()
                
                if modify_time > latest_time:
                    latest_time = modify_time
                    latest_dir = dir_name
        
        if latest_dir:
            latest_path = f"{HADOOP_EMBEDDINGS_DIR}/{latest_dir}"
            logger.info(f"ìµœì‹  ì„ë² ë”© ê²½ë¡œ: {latest_path}")
            return latest_path
        else:
            logger.error("ì„ë² ë”© ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
    
    except Exception as e:
        logger.error(f"í•˜ë‘¡ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# í•˜ë‘¡ì—ì„œ ì„ë² ë”© ë°ì´í„° ë¡œë“œ
def load_embeddings_from_hadoop(spark, embeddings_path):
    """
    í•˜ë‘¡ì—ì„œ ì„ë² ë”© ë°ì´í„° ë¡œë“œ
    
    Args:
        spark (pyspark.sql.SparkSession): ìŠ¤íŒŒí¬ ì„¸ì…˜ ê°ì²´
        embeddings_path (str): ì„ë² ë”© ë°ì´í„° ê²½ë¡œ
        
    Returns:
        tuple: (í…ìŠ¤íŠ¸ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬, ì´ë¯¸ì§€ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬)
    """
    try:
        logger.info(f"í•˜ë‘¡ì—ì„œ ì„ë² ë”© ë°ì´í„° ë¡œë“œ ì¤‘... ({embeddings_path})")
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© íŒŒì¼ ê²½ë¡œ
        text_embeddings_path = f"{embeddings_path}/text_embeddings.json"
        
        # ì´ë¯¸ì§€ ì„ë² ë”© íŒŒì¼ ê²½ë¡œ
        image_embeddings_path = f"{embeddings_path}/image_embeddings.json"
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ë¡œë“œ
        text_embeddings_df = spark.read.text(text_embeddings_path)
        text_embeddings_json = text_embeddings_df.collect()[0][0]
        text_embeddings = json.loads(text_embeddings_json)
        
        # ì´ë¯¸ì§€ ì„ë² ë”© ë¡œë“œ
        image_embeddings_df = spark.read.text(image_embeddings_path)
        image_embeddings_json = image_embeddings_df.collect()[0][0]
        image_embeddings = json.loads(image_embeddings_json)
        
        logger.info(f"í…ìŠ¤íŠ¸ ì„ë² ë”© {len(text_embeddings)}ê°œ, ì´ë¯¸ì§€ ì„ë² ë”© {len(image_embeddings)}ê°œ ë¡œë“œ ì™„ë£Œ")
        
        return text_embeddings, image_embeddings
    
    except Exception as e:
        logger